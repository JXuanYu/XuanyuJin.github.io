---
layout: post
title:  "GLIMPSE: LVLMs Can Watch but Do They Truly Understand?"
date:   2025-03-01 05:50:27 +00:00
image: /images/glimpse.png
categories: research
authors: "Yiyang Zhou, Linjie Li, Shi Qiu, Zhengyuan Yang, Yuyang Zhao, Siwei Han, Yangfan He, Kangqi Li, Haonian Ji, Zihao Zhao, Haibo Tong, Lijuan Wang, Huaxiu Yao"
venue: "under review"
arxiv: 
code: 
website: 
---
Existing video benchmarks often resemble image-based benchmarks, with questions like "What color is the woman’s dress in this video?" or "What’s the weather like in the video?" where models can answer using just a single frame or text, limiting the assessment of large vision language models (LVLMs) in terms of video perception and temporal reasoning abilities. To address this, we introduce GLIMPSE, a benchmark specifically designed to evaluate whether LVLMs truly understand video content. GLIMPSE includes 3,269 videos and over 4,342 highly visual-centric questions across 11 categories, such as Trajectory, Temporal QA, Forensics Detection, and Video Sorting, all meticulously curated and crafted by human annotators. Many of these questions require watching the entire video and reasoning based on video context to answer accurately, as they cannot be addressed using only a few frames or text information. Human testing on GLIMPSE achieves an average accuracy of 94.82%, but GLIMPSE poses significant challenges for existing LVLMs. Even the best-performing model, Gemini 1.5 Pro, reaches only 56.98% accuracy, indicating that current LVLMs still have a long way to go in truly understanding and interpreting video content.